{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR4M5Pn3NXzh"
   },
   "source": [
    "# Ablation Study of DNN Architectures\n",
    "\n",
    "Choosing an architecture is challenging.  In this notebook, we train a fully-connected network with various widths and depths and compare the function approximation quality.  \n",
    "\n",
    "There are many other choices we have not considered and you are welcome to explore on your own:\n",
    "\n",
    "*   Choice of loss function, optimizer, and acitvation function\n",
    "*   Variable widths through the network (e.g., a network that gets wider and then narrower vs. the opposite)\n",
    "* More complicated data (e.g., higher dimensional, more difficult one-dimensional function to approximate, etc.)\n",
    "* and many, many more!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZR4EZ30NQxv"
   },
   "source": [
    "## Step 1: Import Packages\n",
    "\n",
    "We start by importing the necessary packages to run our code.  We are installing the following packages:\n",
    "\n",
    "   * deep learning toolbox [Pytorch](https://pytorch.org/)\n",
    "   * visualization toolbox [Matplotlib](https://matplotlib.org/)\n",
    "   * DNN101 repository [https://github.com/elizabethnewman/dnn101](https://github.com/elizabethnewman/dnn101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de4ULou5Ghie",
    "outputId": "e2b61de7-f373-48ac-d180-375d77de0d4b",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install git+https://github.com/elizabethnewman/dnn101.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHPt_1WLGX5X"
   },
   "outputs": [],
   "source": [
    "import dnn101\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT70M5MTNfOI"
   },
   "source": [
    "## Step 2: Create the Data\n",
    "\n",
    "We will use a 1D regression problem as a simple starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RxOR5E_HOxe"
   },
   "outputs": [],
   "source": [
    "from dnn101.regression import DNN101DataRegression1D\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# data parameters\n",
    "n_train = 2000      # number of training points\n",
    "n_val   = 100       # number of validation points\n",
    "n_test  = 100       # number of test points\n",
    "sigma   = 0.2       # noise level\n",
    "\n",
    "\n",
    "# function to approximate\n",
    "f = lambda x: torch.sin(x)\n",
    "domain  = [-3, 3]   # domain of function\n",
    "dataset = DNN101DataRegression1D(f, domain, noise_level=sigma)\n",
    "\n",
    "# generate data\n",
    "x, y = dataset.generate_data(n_train + n_val + n_test)\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = dataset.split_data(x, y, n_train=n_train, n_val=n_val)\n",
    "\n",
    "# plot data\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['lines.linewidth'] = 8\n",
    "mpl.rcParams['font.size'] = 10\n",
    "\n",
    "dataset.plot_data(x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "plt.show()\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTdL_meRzAHH"
   },
   "source": [
    "### Train with Different Architectures\n",
    "\n",
    "Here, we compare training a network with various widths and depths.  There are many other parameters with which to play, we limit ourselves to only two.  This is an example of how hard it is to determine the optimal architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jS6h2MvbJBEf"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# import training\n",
    "from dnn101.utils import evaluate, train\n",
    "\n",
    "# choose candidate weights and depths\n",
    "widths = 2 ** torch.arange(1, 6)\n",
    "depths = 2 ** torch.arange(0, 6)\n",
    "activation = nn.Tanh()\n",
    "max_iter = 10\n",
    "\n",
    "# storing results\n",
    "results = dict()\n",
    "nets = dict()\n",
    "\n",
    "for i, wi in enumerate(widths):\n",
    "    for j, dj in enumerate(depths):\n",
    "        # set seed for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # build network with different depth and width\n",
    "        layers = (nn.Linear(x_train.shape[1], wi), activation)\n",
    "        for _ in range(dj):\n",
    "            layers += (nn.Linear(wi, wi), activation)\n",
    "        layers += (nn.Linear(wi, y_train.shape[1]),)\n",
    "\n",
    "        net = nn.Sequential(*layers)\n",
    "\n",
    "        # loss and optimizer\n",
    "        loss = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "        # store and print results\n",
    "        values = torch.zeros(max_iter + 1, 3)\n",
    "\n",
    "        loss_train, _ = evaluate(net, loss, (x_train, y_train))\n",
    "        loss_val, _ = evaluate(net, loss, (x_val, y_val))\n",
    "        values[0] = torch.tensor([-1, loss_train, loss_val])\n",
    "        \n",
    "\n",
    "        # ============================================================================ #\n",
    "        for my_iter in range(max_iter):\n",
    "            # training step\n",
    "            loss_running, _ = train(net, loss, (x_train, y_train), optimizer)\n",
    "\n",
    "            # re-evaluate\n",
    "            loss_train, _ = evaluate(net, loss, (x_train, y_train))\n",
    "            loss_val, _ = evaluate(net, loss, (x_val, y_val))\n",
    "\n",
    "            # store and print or plot results\n",
    "            values[my_iter + 1] = torch.tensor([my_iter, loss_train, loss_val])\n",
    "\n",
    "        results[i, j] = values\n",
    "        nets[i, j] = deepcopy(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ol-K7MzN6hXv",
    "outputId": "e78476ca-a7f3-4691-800e-49a992daf869",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plots of results\n",
    "\n",
    "from copy import deepcopy\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (16, 6)\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['lines.linewidth'] = 4\n",
    "\n",
    "C_train = torch.zeros(len(widths), len(depths))\n",
    "C_val = deepcopy(C_train)\n",
    "for i in range(len(widths)):\n",
    "  for j in range(len(depths)):\n",
    "    C_train[i, j] = results[i, j][-1, 1]\n",
    "    C_val[i, j] = results[i, j][-1, 2]\n",
    "\n",
    "\n",
    "img = plt.imshow(torch.log(C_train), origin='lower')\n",
    "cbar = plt.colorbar(img, fraction=0.046, pad=0.04)\n",
    "cbar.ax.set_title('Final Loss (log scale)')\n",
    "plt.xlabel('depth')\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4, 5], labels=['$2^0$', '$2^1$', '$2^2$', '$2^3$', '$2^4$', '$2^5$'])\n",
    "plt.ylabel('width')\n",
    "plt.yticks(ticks=[0, 1, 2, 3, 4], labels=['$2^1$', '$2^2$', '$2^3$', '$2^4$', '$2^5$'])\n",
    "plt.show()\n",
    "\n",
    "img = plt.imshow(torch.log(C_val), origin='lower')\n",
    "cbar = plt.colorbar(img, fraction=0.046, pad=0.04)\n",
    "cbar.ax.set_title('Final Loss (log scale)')\n",
    "plt.xlabel('depth')\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4, 5], labels=['$2^0$', '$2^1$', '$2^2$', '$2^3$', '$2^4$', '$2^5$'])\n",
    "plt.ylabel('width')\n",
    "plt.yticks(ticks=[0, 1, 2, 3, 4], labels=['$2^1$', '$2^2$', '$2^3$', '$2^4$', '$2^5$'])\n",
    "plt.show()\n",
    "\n",
    "# plot convergence\n",
    "for i, wi in enumerate(widths):\n",
    "    plt.subplot(1, len(widths), i + 1)\n",
    "    for j, dj in enumerate(depths):\n",
    "        plt.semilogy(results[i, j][:, 1], label='d=%d' % dj)\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "    plt.title('w = %d' % wi)\n",
    "plt.show()\n",
    "\n",
    "for j, dj in enumerate(depths):\n",
    "    plt.subplot(1, len(depths), j + 1)\n",
    "    for i, wi in enumerate(widths):\n",
    "        plt.semilogy(results[i, j][:, 1], label='w=%d' % wi)\n",
    "      \n",
    "    if j == 0:\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.title('d = %d' % dj)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "AZR4EZ30NQxv"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
