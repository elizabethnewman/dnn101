{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "AZR4EZ30NQxv"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Layers\n",
    "\n",
    "As you become more comfortable with neural networks, you will come up with new ideas and new layers to try. \n",
    "\n",
    "### Outcomes \n",
    "In this tutorial, you will\n",
    "\n",
    "\n",
    "*   learn how to construct a new neural network layer\n",
    "*   learn how to test your new implementation\n",
    "\n",
    "### Suggested Activities\n",
    "* Check out this tutorial on [Extending Autograd](https://pytorch.org/docs/stable/notes/extending.html) and try for yourself!\n",
    "\n",
    "\n",
    "Check out the [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) as you explore!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "fR4M5Pn3NXzh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Import Packages\n",
    "\n",
    "We start by importing the necessary packages to run our code.  We are installing the following packages:\n",
    "\n",
    "   * deep learning toolbox [Pytorch](https://pytorch.org/)\n",
    "   * visualization toolbox [Matplotlib](https://matplotlib.org/)\n",
    "   * DNN101 repository [https://github.com/elizabethnewman/dnn101](https://github.com/elizabethnewman/dnn101)."
   ],
   "metadata": {
    "id": "AZR4EZ30NQxv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m pip install git+https://github.com/elizabethnewman/dnn101.git\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de4ULou5Ghie",
    "outputId": "f0d5c7d0-6bac-4c1c-dc3e-35034c3f5ba3",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KHPt_1WLGX5X"
   },
   "outputs": [],
   "source": [
    "import dnn101\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Create a New Layer\n",
    "\n",
    "We will create a special residual layer of the form\n",
    "\\begin{align*}\n",
    "\\mathbf{z} = \\mathbf{u} + \\tanh(\\mathbf{K}\\mathbf{u})\n",
    "\\end{align*}\n",
    "\n",
    "We base this off the [PyTorch source code for the Linear layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear). \n",
    "\n",
    "We are also going to write our own backwards routine for the features, which mathematically is given by\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{u}} \\mathbf{v} &= \\mathbf{v} + \\mathbf{K}^\\top (\\tanh'(\\mathbf{K}\\mathbf{u}) \\odot \\mathbf{v})\\\\\n",
    "&=\\mathbf{v} + \\mathbf{K}^\\top ((\\mathbf{1} - \\tanh^2(\\mathbf{K}\\mathbf{u})) \\odot \\mathbf{v})\n",
    "\\end{align*}\n",
    "where $\\mathbf{v}$ is the direction in which we apply the directional derivative $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{u}}$ and $\\odot$ is the Hadamard pointwise product. \n",
    "\n",
    "We also want to compute is the gradient with respect to the weights.  This is given by\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\mathbf{z}}{\\partial \\mathbf{K}} \\mathbf{v} = ((\\mathbf{1} - \\tanh^2(\\mathbf{K}\\mathbf{u}))\\odot \\mathbf{v})\\mathbf{u}^\\top\n",
    "\\end{align*}\n",
    "\n",
    "Note that in PyTorch, the data is stored as $(N,H_{\\text{in}})$ where $N$ is the number of samples and $H_{\\text{in}}$ is the number of input features per sample.  We will consider a layer that returns features of size $(N,H_{\\text{out}})$.  This means the mathematical operations will be applied from the right and transposed when we implement."
   ],
   "metadata": {
    "id": "FT70M5MTNfOI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# here is a simple way to implement this layer using automatic differentation\n",
    "class SpecialResidualLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.weight = nn.Parameter(torch.empty((in_features, in_features), **factory_kwargs))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.tanh(x @ self.weight.T)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, activation={}'.format(\n",
    "            self.in_features, self.activation\n",
    "        )\n",
    "\n",
    "\n",
    "# here is a way to implement your layer with a self-made backward\n",
    "class SpecialResidualFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(u, K):\n",
    "        # note that we multiply from the right by python conventions \n",
    "        z = u + torch.tanh(u.mm(K.t()))\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        u, K = inputs\n",
    "        ctx.save_for_backward(u, K)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, v):\n",
    "        u, K = ctx.saved_tensors\n",
    "        grad_u = grad_K = None\n",
    "        if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n",
    "            grad_a = (1 - torch.tanh(u @ K.t()) ** 2) * v\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_u = v + (grad_a).mm(K)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_K = (grad_a.t()).mm(u)\n",
    "\n",
    "        return grad_u, grad_K\n",
    "\n",
    "\n",
    "class SpecialResidualLayer2(nn.Module):\n",
    "    def __init__(self, in_features: int, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.weight = nn.Parameter(torch.empty((in_features, in_features), **factory_kwargs))\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return SpecialResidualFunction.apply(x, self.weight)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}'.format(\n",
    "            self.in_features\n",
    "        )\n"
   ],
   "metadata": {
    "id": "_RxOR5E_HOxe"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Test the Layer\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "2OJhxaR9vh7O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 1: Use ```autograd``` to check\n",
    "\n",
    "This uses the method of finite differences."
   ],
   "metadata": {
    "id": "Lt7jnUb3I6-h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# first, how do we test with Pytorch\n",
    "func = SpecialResidualFunction.apply\n",
    "\n",
    "u = torch.randn(11, 3, requires_grad=True)\n",
    "K = torch.randn(3, 3, requires_grad=True)\n",
    "torch.autograd.gradcheck(func, (u, K))"
   ],
   "metadata": {
    "id": "8MiY_qHbISkZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 2: Use Taylor series\n",
    "\n",
    "We describe use a Taylor approximation approach to test the derivative for the weights. \n",
    "\n",
    "Suppose we have a smooth, scalar-valued objective function $f$.  We can expand about the weights $\\boldsymbol{\\theta}$ of unit length in the direction $\\mathbf{v}$ using Taylor series as follows:\n",
    "\\begin{align*}\n",
    "f(\\boldsymbol{\\theta} + h \\mathbf{v}) = f(\\boldsymbol{\\theta}) + h\\langle \\nabla f(\\boldsymbol{\\theta}), \\mathbf{v}\\rangle + \\mathcal{O}(h^2)\n",
    "\\end{align*}\n",
    "\n",
    "If we compute the gradient correctly, then as $h\\to 0$, the absolute error of the linear approximation\n",
    "\\begin{align*}\n",
    "| f(\\boldsymbol{\\theta}) + h\\langle \\nabla f(\\boldsymbol{\\theta}), \\mathbf{v}\\rangle - f(\\boldsymbol{\\theta} + h \\mathbf{v})|\n",
    "\\end{align*}\n",
    "will decay on the order of $h^2$.  This is what we want to observe in our layer."
   ],
   "metadata": {
    "id": "iCBV-SOsJA3G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from dnn101.utils import convert_to_base\n",
    "\n",
    "# set data type for better\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# set seed for\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# layer or network to test\n",
    "in_features = 3\n",
    "layer = SpecialResidualLayer2(in_features)\n",
    "\n",
    "# create data and forward propagate\n",
    "x = torch.randn(11, in_features, requires_grad=True) # this will pass through our backward\n",
    "y = layer(x)\n",
    "\n",
    "# choose loss function (any will do!)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# compute evaluation without perturbations\n",
    "y_true = torch.randn_like(y)\n",
    "out = loss(y, y_true)\n",
    "\n",
    "# compute gradients\n",
    "out.backward()\n",
    "\n",
    "# choose variable to test\n",
    "perturb_w = True\n",
    "\n",
    "if perturb_w:\n",
    "  # fix features (every torch.Tensor has a data and grad attribute)\n",
    "  w, dw = deepcopy(layer.weight.data), deepcopy(layer.weight.grad)\n",
    "else:\n",
    "  x, dx = deepcopy(x.data), deepcopy(x.grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # form perturbation and compute inner product with\n",
    "\n",
    "    if perturb_w:\n",
    "      p = torch.randn_like(dw)\n",
    "      pgrad = (p * dw).sum()\n",
    "    else:\n",
    "      p = torch.randn_like(dx)\n",
    "      pgrad = (p * dx).sum()\n",
    "\n",
    "    # MAIN ITERATION\n",
    "    headers = ('h', 'E0', 'E1')\n",
    "    print(('{:<20s}' * len(headers)).format(*headers))\n",
    "\n",
    "    num_test = 15\n",
    "    E0, E1 = torch.zeros(num_test), torch.zeros(num_test)\n",
    "    for k in range(num_test):\n",
    "        # step size\n",
    "        h = 2.0 ** (-k)\n",
    "\n",
    "        # perturb weights and forward propgate\n",
    "        if perturb_w:\n",
    "          layer.weight = nn.Parameter(w + h * p)\n",
    "          y_h = layer(x)\n",
    "        else:\n",
    "          y_h = layer(x + h * p)\n",
    "\n",
    "        # evaluate\n",
    "        out_h = loss(y_h, y_true)\n",
    "\n",
    "        # compute loss\n",
    "        err0 = torch.norm(out - out_h)\n",
    "        err1 = torch.norm(out + h * pgrad - out_h)\n",
    "\n",
    "        printouts = convert_to_base((err0, err1))\n",
    "        print(((1 + len(printouts) // 2) * '%0.2f x 2^(%0.2d)\\t\\t') % ((1, -k) + printouts))\n",
    "\n",
    "        E0[k] = err0.item()\n",
    "        E1[k] = err1.item()\n",
    "\n",
    "    tol = 0.1\n",
    "    eps = torch.finfo(x.dtype).eps\n",
    "    grad_check = (sum((torch.log2(E1[:-1] / E1[1:])) > (2 - tol)) > 3)\n",
    "    grad_check = (grad_check or (torch.kthvalue(E1, num_test // 3)[0] < (100 * eps)))\n",
    "\n",
    "    if grad_check:\n",
    "        print('Gradient PASSED!')\n",
    "    else:\n",
    "        print('Gradient FAILED.')"
   ],
   "metadata": {
    "id": "Qm3x6zvAP-8q"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
