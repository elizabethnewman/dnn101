{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "AZR4EZ30NQxv",
    "FT70M5MTNfOI",
    "xTWZ8_S1xsOy",
    "4QKhWYC-xwMt",
    "7LBjcyvxy8pZ",
    "XTdL_meRzAHH",
    "BmYdrBO6cm90"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regression\n",
    "\n",
    "Regression is a function approximation problem in which we seek to learn a mapping such that $f(\\mathbf{y},\\boldsymbol{\\theta}) \\approx \\mathbf{c}$ for all input-target pairs $(\\mathbf{y},\\mathbf{c}) \\in \\mathcal{D}$.  Here, $\\mathbf{y}$ represents the inputs (e.g., parameters) and $\\mathbf{c}$ represents values of an unknown mapping (e.g., measurements, observations, etc.)\n",
    "\n",
    "### Outcomes \n",
    "In this tutorial, you will\n",
    "\n",
    "*   generate regression test problems to approximate one- or two-dimensional functions\n",
    "*   learn how to construct a neural network using PyTorch\n",
    "*   train a network to fit your function using stochastic optimization\n",
    "\n",
    "### Suggested Activities\n",
    "\n",
    "*  How does the network architecture affect the performance?  Play with the width, depth, and activation functions and see if there are differences in approximation quality and training challenges.  \n",
    "*  How does the data affect performance?  What if we have fewer training data points?  Do we fit better?  How does the noise level affect our performance?\n",
    "*  How does the optimizer affect performance?  Try different optimizer parameters (e.g., learning rate, momentum, etc.).  Compare different optimizers and see which ones train best.\n",
    "\n",
    "* **Challenges:** \n",
    "\n",
    "  * How does the neural network approximation compare to traditional data fitting approaches? Is polynomial approximation better?\n",
    "  * How well do we generalize (in this case, extrapolate) outside of the domain?\n",
    "  * What if the function we seek to approximate is outside of the hypothesis space of functions (e.g., if we approximate a rational function)?  Can a neural network capture this behavior?  \n",
    "  * We often try to add regularization to avoid fitting noise.  Try to implement Tikhonov regularization from scratch.  This is encoded in the optimizers with the ```weight_decay``` parameter, which gives you some to which to compare your implmentation.\n",
    "\n",
    "Check out the [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) as you explore!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "fR4M5Pn3NXzh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Import Packages\n",
    "\n",
    "We start by importing the necessary packages to run our code.  We are installing the following packages:\n",
    "\n",
    "   * deep learning toolbox [Pytorch](https://pytorch.org/)\n",
    "   * visualization toolbox [Matplotlib](https://matplotlib.org/)\n",
    "   * DNN101 repository [https://github.com/elizabethnewman/dnn101](https://github.com/elizabethnewman/dnn101)."
   ],
   "metadata": {
    "id": "AZR4EZ30NQxv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m pip install git+https://github.com/elizabethnewman/dnn101.git"
   ],
   "metadata": {
    "id": "de4ULou5Ghie"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KHPt_1WLGX5X"
   },
   "outputs": [],
   "source": [
    "import dnn101\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Create the Data\n",
    "\n",
    "We provide two approaches to creating data:\n",
    "1. **DNN101DataRegression1D**: fit a one-dimensional function $f: [a_1, b_1] \\to \\mathbb{R}$\n",
    "2. **DNN101DataRegression2D**: fit a two-dimensional function $f: [a_1, b_1] \\times [a_2, b_2] \\to \\mathbb{R}$\n",
    "\n",
    "The user provides a callable function $f$; e.g., \n",
    "```python\n",
    "f = lambda x: x ** 2                      # one-dimensional\n",
    "f = lambda x: x[:,0] ** 2 + x[:, 1] ** 2  # two-dimensional\n",
    "```\n",
    "\n",
    "The **DNN101Data** environments provide methods to plot the data and the network predictions.\n"
   ],
   "metadata": {
    "id": "FT70M5MTNfOI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from dnn101.regression import DNN101DataRegression1D, DNN101DataRegression2D\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# data parameters\n",
    "n_train = 2000      # number of training points\n",
    "n_val   = 100       # number of validation points\n",
    "n_test  = 100       # number of test points\n",
    "sigma   = 0.2       # noise level\n",
    "\n",
    "\n",
    "# function to approximate\n",
    "f = lambda x: torch.sin(x)\n",
    "domain  = (-3, 3)   # domain of function\n",
    "dataset = DNN101DataRegression1D(f, domain, noise_level=sigma)\n",
    "\n",
    "# f = lambda x: torch.sin(x[:, 0]) + torch.cos(x[:, 1])\n",
    "# domain  = [-3, 3, -3, 3]   # domain of function\n",
    "# dataset = DNN101DataRegression2D(f, domain, noise_level=sigma)\n",
    "\n",
    "# generate data\n",
    "x, y = dataset.generate_data(n_train + n_val + n_test)\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = dataset.split_data(x, y, n_train=n_train, n_val=n_val)\n",
    "\n",
    "# plot data\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['lines.linewidth'] = 8\n",
    "mpl.rcParams['font.size'] = 10\n",
    "\n",
    "dataset.plot_data((x_train, y_train), (x_val, y_val), (x_test, y_test))\n",
    "plt.show()\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ],
   "metadata": {
    "id": "_RxOR5E_HOxe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Train!\n",
    "\n",
    "This block is the heart of DNN training and consists of four main steps\n",
    "1. Define the architecture of the DNN (for this notebook, we choose [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers) and [Activation Functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity))\n",
    "2. Choose a loss function ([PyTorch Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)).  For regression, we use the [Mean Squared Error Loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss). \n",
    "3. Choose an optimizer ([PyTorch Optimizers](https://pytorch.org/docs/stable/optim.html?highlight=optim#torch.optim.Optimizer))\n",
    "4. Train with stochastic optimization.  \n",
    "\n",
    "If you change parameters, be sure to run all blocks in this section to make sure everything is connected properly."
   ],
   "metadata": {
    "id": "2OJhxaR9vh7O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Architecture\n",
    "\n",
    "We will construct a fully-connected neural network using [linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) and [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). \n",
    "\n",
    "Note that in PyTorch, a linear layer is, by default, an affine transformation."
   ],
   "metadata": {
    "id": "xTWZ8_S1xsOy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 14),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(14, 10),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, y_train.shape[1])\n",
    ")"
   ],
   "metadata": {
    "id": "ipXB5NITxrmR"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Loss Function\n",
    "\n",
    "For function approximation, our goal is to minimize the expected least squares loss given by\n",
    "\\begin{align*}\n",
    "\\min_{\\boldsymbol{\\theta}} \\mathbb{E}\\|F(\\mathbf{y}, \\boldsymbol{\\theta}) - \\mathbf{c}\\|_2^2\n",
    "\\end{align*}\n",
    "\n",
    "Note: in PyTorch, the default [mean squared error loss function](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)  is given by\n",
    "\\begin{align*}\n",
    "\\frac{1}{n_b}\\sum_{i=1}^{n_b} \\tfrac{1}{m}\\|\\mathbf{z}_i - \\mathbf{c}_i\\|_2^2\n",
    "\\end{align*}\n",
    "where $n_b$ is the batch size and $m$ is the number of target features.  Effectively, the loss averages over all of the entries rahter than all of the samples."
   ],
   "metadata": {
    "id": "4QKhWYC-xwMt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loss = torch.nn.MSELoss()"
   ],
   "metadata": {
    "id": "ub4fYRsNxyjd"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Optimizer\n",
    "\n",
    "There are many choices of [PyTorch optimizers](https://pytorch.org/docs/stable/optim.html).  We select the popular Adam (Adapative Momentum estimation) method."
   ],
   "metadata": {
    "id": "7LBjcyvxy8pZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "id": "zi-viz3Ny_w0"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Train!"
   ],
   "metadata": {
    "id": "XTdL_meRzAHH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython import display\n",
    "mpl.rcParams['figure.figsize'] = (16, 6)\n",
    "mpl.rcParams['lines.linewidth'] = 8\n",
    "mpl.rcParams['font.size'] = 10\n",
    "\n",
    "# import training\n",
    "from dnn101.utils import evaluate\n",
    "\n",
    "# printing and plotting options (only one can be True)\n",
    "verbose = True             # printouts\n",
    "show_plot = not verbose     # plot to show training\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# store and print results\n",
    "info = {\n",
    "    'headers': ('epoch', 'running_loss', 'training_loss', 'validation_loss'),\n",
    "    'formats': '{:<15d}{:<15.4e}{:<15.4e}{:<15.4e}',\n",
    "    'values': None\n",
    "    }\n",
    "\n",
    "loss_train, _ = evaluate(net, loss, (x_train, y_train))\n",
    "loss_val, _ = evaluate(net, loss, (x_val, y_val))\n",
    "info['values'] = [[-1, 0.0, loss_train, loss_val]]\n",
    "\n",
    "if verbose:\n",
    "    print(('{:<15s}' * len(info['headers'])).format(*info['headers']))\n",
    "    print(info['formats'].format(*info['values'][-1]))\n",
    "\n",
    "# ============================================================================ #\n",
    "# OUTER ITERATION\n",
    "# ============================================================================ #\n",
    "max_epochs = 20\n",
    "batch_size = 5\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    # ======================================================================== #\n",
    "    # INNER ITERATION (update for all batches in one epoch)\n",
    "    # ======================================================================== #\n",
    "    n_batch = x_train.shape[0] // batch_size\n",
    "    shuffle_idx = torch.randperm(x_train.shape[0])\n",
    "    running_loss = 0.0\n",
    "    for i in range(n_batch):\n",
    "        # select batch\n",
    "        idx = shuffle_idx[i * batch_size:(i + 1) * batch_size]\n",
    "        xb, yb = x_train[idx], y_train[idx]\n",
    "\n",
    "        # zero out gradients (very important step!)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward propagate\n",
    "        yb_hat = net(xb)\n",
    "\n",
    "        # evaluate (with average loss)\n",
    "        phi = loss(yb_hat, yb)\n",
    "        running_loss += yb.numel() * phi.item()\n",
    "\n",
    "        # backward propagate (with automatic differentiation)\n",
    "        phi.backward()\n",
    "\n",
    "        # update (with optimizer rule)\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluate performance for each epoch\n",
    "    loss_running = running_loss / (n_batch * batch_size)\n",
    "    loss_train, _ = evaluate(net, loss, (x_train, y_train))\n",
    "    loss_val, _ = evaluate(net, loss, (x_val, y_val))\n",
    "\n",
    "    # store and print or plot results\n",
    "    info['values'].append([epoch, loss_running, loss_train, loss_val])\n",
    "    if verbose:\n",
    "        print(info['formats'].format(*info['values'][-1]))\n",
    "\n",
    "    # plot function approximation and loss\n",
    "    if show_plot:\n",
    "      with torch.no_grad():\n",
    "        plt.subplot(1, 2, 1)\n",
    "        dataset.plot_prediction(dataset.f)\n",
    "        dataset.plot_prediction(net, label='pred', color='g', linestyle='--')\n",
    "        plt.title('approx: epoch = %d' % epoch)\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        values = torch.tensor(info['values'])\n",
    "\n",
    "        plt.semilogy(values[:, 0], values[:, 2], label='train')\n",
    "        plt.semilogy(values[:, 0], values[:, 3], '--', label='validation')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlim([-1, max_epochs])\n",
    "        plt.ylim([1e-2, 1e0])\n",
    "        plt.legend()\n",
    "        plt.title('convergence: test loss = %0.4e' % evaluate(net, loss, (x_test, y_test))[0])\n",
    "\n",
    "\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ],
   "metadata": {
    "id": "jS6h2MvbJBEf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Inference\n",
    "\n",
    "Let's assess how well our neural network performs on unseen data (i.e., the test data) and how well it extrpolates outside of the domain on which we trained."
   ],
   "metadata": {
    "id": "BmYdrBO6cm90"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# final losses\n",
    "loss_train, _ = evaluate(net, loss, (x_train, y_train))\n",
    "loss_val, _ = evaluate(net, loss, (x_val, y_val))\n",
    "loss_test, _ = evaluate(net, loss, (x_test, y_test))\n",
    "\n",
    "print('Train Loss = %0.4e' % loss_train)\n",
    "print('Valid Loss = %0.4e' % loss_val)\n",
    "print('Test Loss = %0.4e' % loss_test)"
   ],
   "metadata": {
    "id": "YGf2QICNc0mD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# extrapolation\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 8\n",
    "mpl.rcParams['font.size'] = 10\n",
    "\n",
    "# create larger domain (extend by n_ext in every direction)\n",
    "n_ext = 4\n",
    "\n",
    "if isinstance(dataset, DNN101DataRegression1D):\n",
    "    mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "    domain_ext = (domain[0] - n_ext, domain[1] + n_ext)\n",
    "    dataset_ext = DNN101DataRegression1D(f, domain_ext, noise_level=sigma)\n",
    "    \n",
    "    plt.vlines(domain[0], -1, 1, 'k', ':', label='domain')\n",
    "    plt.vlines(domain[1], -1, 1, 'k', ':')\n",
    "\n",
    "    # plot prediction\n",
    "    dataset_ext.plot_prediction(dataset.f)\n",
    "    dataset_ext.plot_prediction(net, label='pred', color='g', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    mpl.rcParams['figure.figsize'] = (16, 6)\n",
    "    domain_ext = (domain[0] - n_ext, domain[1] + n_ext, \n",
    "                  domain[2] - n_ext, domain[3] + n_ext)\n",
    "    dataset_ext = DNN101DataRegression2D(f, domain_ext, noise_level=sigma)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    dataset_ext.plot_prediction(dataset.f)\n",
    "    plt.vlines(domain[0], domain[2], domain[3], 'k')\n",
    "    plt.vlines(domain[1], domain[2], domain[3], 'k')\n",
    "    plt.hlines(domain[2], domain[0], domain[1], 'k')\n",
    "    plt.hlines(domain[3], domain[0], domain[1], 'k')\n",
    "    plt.title('true')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    dataset_ext.plot_prediction(net)\n",
    "    plt.vlines(domain[0], domain[2], domain[3], 'k')\n",
    "    plt.vlines(domain[1], domain[2], domain[3], 'k')\n",
    "    plt.hlines(domain[2], domain[0], domain[1], 'k')\n",
    "    plt.hlines(domain[3], domain[0], domain[1], 'k')\n",
    "    plt.title('prediction')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    dataset_ext.plot_prediction1(lambda x: torch.abs(net(x) - dataset.f(x)))\n",
    "    plt.vlines(domain[0], domain[2], domain[3], 'k')\n",
    "    plt.vlines(domain[1], domain[2], domain[3], 'k')\n",
    "    plt.hlines(domain[2], domain[0], domain[1], 'k')\n",
    "    plt.hlines(domain[3], domain[0], domain[1], 'k')\n",
    "    plt.title('abs. diff.')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ],
   "metadata": {
    "id": "kqN8m7hyhSxA"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
